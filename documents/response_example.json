{ 
    "id": "chatcmpl-8I5u0zwluai4pLP4iJL6jKn23SIqs", 
    "object": "chat.completion", 
    "created": 1699323284, 
    "model": "gpt-3.5-turbo-0613", 
    "choices": [ { 
        "index": 0, 
        "message": { 
            "role": "assistant", 
            "content": "Language documentation is about recording how people speak. Linguists transcribe these recordings, which means writing down what is said word for word. Transcribing takes a long time and can be difficult. This can cause a problem because there is a risk that recordings will not be transcribed or translated. People are working on finding better ways for linguists to do their work. One idea is to use speech recognition technology, which can automatically transcribe what is said. However, this technology has not worked well for endangered languages. This is because the technology usually needs a lot of data and text to train on, which is not always available for endangered languages. In this paper, researchers explore using a different type of speech recognition model called connectionist temporal classification (CTC). This model can predict phonemes and tones directly from the sound of someone speaking. The researchers hope that using this model will make it easier and faster to transcribe endangered languages." 
        },
        "finish_reason": "stop" 
    } ], 
    "usage": { 
        "prompt_tokens": 701, 
        "completion_tokens": 187, 
        "total_tokens": 888 
    } 
}


{
    "model": "gpt-3.5-turbo",
    "messages": [
        {
            "role": "system",
            "content": "Writing style guideline: Write a brief summary at a 6th grade level. Use clear and simple language, even when explaining complex topics. Bias toward short sentences. Avoid jargon."
        },
        {
            "role": "user",
            "content": "Language documentation involves recording the speech of native speakers. Transcribing these recordings, which are rich cultural and linguistic resources, is an integral part of the language documentation process. However, transcription is slow: it often takes a linguist between 30 minutes to 2 hours to transcribe and translate a minute of speech, depending on the transcriber’s familiarity with the language and the difficulty of the content. This is a bottleneck in the documentary linguistics workflow: linguists accumulate considerable amounts of speech, but do not transcribe and translate it all, and there is a risk that untranscribed recordings could end up as “data graveyards” (Himmelmann, 2006, 4,12-13). There is clearly a need for “devising better ways for linguists to do their work” (Thieberger, 2016, 92). There has been work on low-resource speech recognition (Besacier et al., 2014), with approaches using cross-lingual information for better acoustic modelling (Burget et al., 2010; Vu et al., 2014; Xu et al., 2016; Müller et al., 2017) and language modelling (Xu and Fung, 2013). However, speech recognition technology has largely been ineffective for endangered languages since architectures based on hidden Markov models (HMMs), which generate orthographic transcriptions, require a large pronunciation lexicon and a language model trained on text. These speech recognition systems are usually trained on a variety of speakers and hundreds of hours of data (Hinton et al., 2012, 92), with the goal of generalisation to new speakers. Since large amounts of text are used for language model training, such systems often do not incorporate pitch information for speech recognition of tonal languages (Metze et al., 2013), as they can instead rely on contextual information for tonal disambiguation via the language model (Le and Besacier, 2009; Feng et al., 2012) even though there is no computational burden in additionally using pitch features. In contrast, language documentation contexts often have just a few speakers for model training and little text for language model training. However, there may be benefit even in a system that overfits to these speakers. If a phonemic recognition tool can provide a canvas transcription for manual correction and linguistic analysis, it may be possible to improve the workflow and leverage of linguists. The transcriptions collected in this semi-automated workflow can then be used for refinement of the acoustic model, leading to a snowball effect of better and faster transcription. In this paper we investigate the application of neural speech recognition models to the task of phonemic and tonal transcription in a language documentation setting where limited resources are available for model training. We use the connectionist temporal classification (CTC) formulation (Graves et al., 2006) for the purposes of direct prediction of phonemes and tones given an acoustic signal, thus bypassing the need for a pronunciation lexicon, language model, and time alignments of phonemes in the training data. By reducing the data requirements we make automatic transcription technology more feasible in a language documentation setting."
        }
    ]
}